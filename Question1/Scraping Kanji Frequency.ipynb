{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Dustin Seltz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "\n",
    "Scrape frequency data of different sources from https://scriptin.github.io/kanji-frequency/\n",
    "\n",
    "Input:\n",
    "\n",
    "None. \n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "KanjiFrequencyOnWikipedia.csv\n",
    "\n",
    "KanjiFrequencyOnNews.csv\n",
    "\n",
    "KanjiFrequencyOnTwitter.csv\n",
    "\n",
    "KanjiFrequencyOnAozora.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There's probably an easier way to load data from JSON, \n",
    "#    but I could use more practice with web scraping anyway. \n",
    "#(I scraped this data before we covered anything about JSON)\n",
    "sourceNames = [\"Aozora\", \"News\", \"Twitter\", \"Wikipedia\"]\n",
    "urlsToScrape = []\n",
    "for name in sourceNames:\n",
    "    name = name.lower()\n",
    "    urlsToScrape.append(\"https://raw.githubusercontent.com/scriptin/kanji-frequency/master/data/\"+name+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmls = [urlopen(urlToScrape) for urlToScrape in urlsToScrape]\n",
    "soups = [BeautifulSoup(html, 'lxml') for html in htmls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiSoup = soups[3]\n",
    "wikiSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing regex on this stuff\n",
    "str1 = \"\"\"[\"å¹´\",21066593,0.02685131991368414],\"\"\"\n",
    "expr = \"\"\"\\[\"(.)\",(.*),(.*)\\]\"\"\"\n",
    "match = re.match(expr, str1)\n",
    "if(match):\n",
    "    print(match.group())\n",
    "    print(match.group(1))\n",
    "    print(match.group(2))\n",
    "    print(match.group(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape from the soup\n",
    "expr = \"\"\"\\[\"(.)\",(.*),(.*)\\]\"\"\"\n",
    "paragraph = wikiSoup.find_all(\"p\")[0].text\n",
    "paragraphLines = paragraph.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = [re.findall(expr, line) for line in paragraphLines]\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn the result into something we can easily make into a dataframe\n",
    "matchesList = []\n",
    "for entry in matches:\n",
    "    #Each entry is a list of length 1. \n",
    "    for tup in entry:\n",
    "        if(len(tup) == 3):\n",
    "            character = tup[0]\n",
    "            numberOfAppearances = tup[1]\n",
    "            percentage = tup[2]\n",
    "            matchesList.append([character, numberOfAppearances, percentage])\n",
    "matchesList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the data in a dataframe\n",
    "colNames = [\"Character\", \"Number of Appearances\", \"%\"]\n",
    "df = pd.DataFrame(matchesList, columns=colNames)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the dataframe in a file\n",
    "file_name = \"KanjiFrequencyOnWikipedia\"\n",
    "df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test that it worked\n",
    "df = pd.read_csv(file_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks good, but I want to make all four output files at once. Lets do all that stuff but in a loop.\n",
    "for (soup,name) in zip(soups, sourceNames):\n",
    "    #Scrape from the soup\n",
    "    expr = \"\"\"\\[\"(.)\",(.*),(.*)\\]\"\"\"\n",
    "    paragraph = soup.find_all(\"p\")[0].text\n",
    "    paragraphLines = paragraph.splitlines()\n",
    "    matches = [re.findall(expr, line) for line in paragraphLines]\n",
    "    #Turn the result into something we can easily make into a dataframe\n",
    "    matchesList = []\n",
    "    for entry in matches:\n",
    "        #Each entry is a list of length 1. \n",
    "        for tup in entry:\n",
    "            if(len(tup) == 3):\n",
    "                character = tup[0]\n",
    "                numberOfAppearances = tup[1]\n",
    "                percentage = tup[2]\n",
    "                matchesList.append([character, numberOfAppearances, percentage])\n",
    "    #Store the data in a dataframe\n",
    "    colNames = [\"Character\", \"Number of Appearances\", \"%\"]\n",
    "    df = pd.DataFrame(matchesList, columns=colNames)\n",
    "    #Store the dataframe in a file\n",
    "    file_name = \"KanjiFrequencyOn\"+name\n",
    "    df.to_csv(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
