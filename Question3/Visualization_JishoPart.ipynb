{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of Creator\n",
    "CREATOR_NAME = \"Jingheng Wang\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is intended to make some observations on data generated from Jisho, focusing on the following topics:\n",
    "1. Number of strokes (X) - rank of frequency in news (Y)\n",
    "2. Difficulty Level (X) - rank of frequency in news (Y)\n",
    "3. Group of most frequently used 20 Radicals (X) - rank of frequency in news (Y)\n",
    "4. Difficulty Level (grades and JLPT, X) - number of Kanjis in that level (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'cleaned_link.csv' does not exist: b'cleaned_link.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b85ad289a5a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cleaned_link.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'cleaned_link.csv' does not exist: b'cleaned_link.csv'"
     ]
    }
   ],
   "source": [
    "# initialization, read csv file\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "raw = pd.read_csv(\"../Question1/cleaned_link.csv\")\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of strokes (X) - rank of frequency, average (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE BOX PLOT, CALCULATE MAX/MIN\n",
    "\n",
    "# Initialization\n",
    "container1 = [np.nan]\n",
    "\n",
    "stroke_char_count = [np.nan]\n",
    "\n",
    "# For each # of stroke (1~29)\n",
    "for strnum in np.arange(1,30):\n",
    "    \n",
    "    # Find all rows where strokes equal to the current loop number\n",
    "    raw_in_strnum = raw[raw[\"strokes\"] == strnum]\n",
    "    \n",
    "    # Calculate the number of kanjis with that stroke (where frequency data != NaN), append to count array\n",
    "    stroke_char_count.append(raw_in_strnum[\"frequency\"].count())\n",
    "    \n",
    "    # Append those kanji's frequency ranks to a container\n",
    "    container1.append(list(raw_in_strnum[\"frequency\"].dropna()))\n",
    "    \n",
    "# The container has (ideally) 29 lists, each contains the rank of frequencies of kanjis with corresponding strokes.\n",
    "# So container[1] is kanjis with 1 stroke, their frequency ranks\n",
    "container1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the characters do not have a frequency number. That's why although the maximum is 29, some of them are NaNs.\n",
    "raw[raw[\"strokes\"] == 29]\n",
    "\n",
    "# So focus on the meaningful values, strokes from 1 to 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "# Add figure, axis\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "axis = fig.add_subplot(1,1,1)\n",
    "\n",
    "# Box plot\n",
    "axis.boxplot(container1[1:24])\n",
    "\n",
    "# Title, xlabel, ylabel\n",
    "axis.set_title(\"number of strokes (X) - rank of frequency(Y)\", fontsize=\"large\")\n",
    "axis.set_xlabel(\"number of strokes (count of kanjis)\", fontsize=\"large\")\n",
    "axis.set_ylabel(\"rank of frequency used in newspaper\", fontsize=\"large\")\n",
    "\n",
    "# Generate xticks and its labels\n",
    "xticks = []\n",
    "for i in np.arange(1,24):\n",
    "    xticks.append(\"{} ({})\".format(i, stroke_char_count[i]))\n",
    "    \n",
    "axis.set_xticks(np.arange(1,24))\n",
    "axis.set_xticklabels(xticks,rotation=30)\n",
    "\n",
    "# Flip the box y-axis\n",
    "axis.set_ylim(axis.get_ylim()[::-1])\n",
    "\n",
    "# useless, avoid output from set_xticklabels\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In the box plot, the box indicates the Q1 and Q3 quartiles.\n",
    "Although the range of Kanjis frequencies don't have big difference, the Q1 and Q3 quartiles shows some interesting data: for kanjis that have less strokes, that box seems to be higher than those with more strokes, indicating that they are more frequently used. So, generally, kanjis with less strokes seems to appear more frequent than those of more strokes.\n",
    "\n",
    "The special case for \"stroke = 1\" only contains 2 kanjis, the sample size is too small, thus we can ignore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difficulty Level (X) - rank of frequency, average (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many different grades in our DF\n",
    "raw_grades = raw[\"grade\"].unique()\n",
    "# sort every value except NaN, which happens to be the last element\n",
    "grades = np.sort(raw_grades[:-1])\n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization. Container 2 is the list of \"list of frequencies of kanjis taught in some grade\"\n",
    "container2 = []\n",
    "\n",
    "grade_char_count = []\n",
    "\n",
    "# For all grades\n",
    "for gradenum in np.arange(len(grades)):\n",
    "    \n",
    "    # locate all rows with that grade\n",
    "    raw_in_gradenum = raw[raw[\"grade\"] == grades[gradenum]]\n",
    "\n",
    "    # count the number of kanjis taught in that grade\n",
    "    grade_char_count.append(raw_in_gradenum[\"frequency\"].count())\n",
    "    \n",
    "    # append the list of frequencies to containers\n",
    "    container2.append(list(raw_in_gradenum['frequency'].dropna()))\n",
    "    \n",
    "container2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization, figure and axis\n",
    "fig2 = plt.figure(figsize=(12,9))\n",
    "axis21 = fig2.add_subplot(1,1,1)\n",
    "\n",
    "# Box plot\n",
    "axis21.boxplot(container2)\n",
    "\n",
    "# Title, xlabel, ylabel\n",
    "axis21.set_title(\"Grades (X) - rank of frequency(Y)\", fontsize=\"large\")\n",
    "axis21.set_xlabel(\"Grades (count of kanjis in grade)\", fontsize=\"large\")\n",
    "axis21.set_ylabel(\"rank of frequency used in newspaper\", fontsize=\"large\")\n",
    "\n",
    "# generate and set xticks and labels\n",
    "xticks21 = []\n",
    "for i in np.arange(len(grades)):\n",
    "    xticks21.append(\"{} ({})\".format(grades[i], grade_char_count[i]))\n",
    "    \n",
    "axis21.set_xticks(np.arange(1,len(grades)+1))\n",
    "axis21.set_xticklabels(xticks21,rotation=30)\n",
    "\n",
    "# Flip the box y-axis\n",
    "axis21.set_ylim(axis21.get_ylim()[::-1])\n",
    "\n",
    "# Avoid output from set_xticklabels\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This graph actually is easier to interpret than the previous one. Apparently, as the students go into higher grades, they study kanjis that are much less frequently used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group of most frequently used 20 Radicals (X) - rank of frequency (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all radicals from the table\n",
    "raw_radicals = [ast.literal_eval(res).keys() for res in raw['radicals'].unique()]\n",
    "\n",
    "# pick only the first character of those radicals\n",
    "radicals = [list(x)[0][0] for x in raw_radicals]\n",
    "radicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "container3 = {}\n",
    "radical_char_count = []\n",
    "\n",
    "# For each radical\n",
    "for rad in np.arange(len(radicals)):\n",
    "    \n",
    "    # Locate all rows with that radical\n",
    "    raw_in_radnum = raw[[(x.find(radicals[rad]) != -1) for x in raw['radicals']]]\n",
    "\n",
    "    # Count the number of kanjis with that radical\n",
    "    radical_char_count.append(raw_in_radnum[\"radicals\"].count())\n",
    "    \n",
    "    # Append to container dictionary. Key is the radical, value is a list of frequencies of kanjis with that radical\n",
    "    container3[radicals[rad]]=list(raw_in_radnum['frequency'].dropna())\n",
    "  \n",
    "#\n",
    "# Combine them into 2-tuples\n",
    "container32 = [(x,container3[x]) for x in container3.keys()]\n",
    "\n",
    "# Sorting function\n",
    "def sort_key(x):\n",
    "    a,b = x\n",
    "    return -len(b)\n",
    "\n",
    "f=sort_key\n",
    "\n",
    "# Sort the list\n",
    "container32.sort(key=f)\n",
    "\n",
    "# Unzip them into separate lists\n",
    "rad_sorted, freq_sorted = zip(*container32)\n",
    "#freq_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization needed Initialization\n",
    "# Normally, Radicals cannot be correctly displayed in the matplotlib (no normal fonts support the radicals)\n",
    "# So it is required to use some extra fonts\n",
    "# The suggested font, \"simhei.ttf\", can be downloaded here: https://www.fontpalace.com/font-download/SimHei/\n",
    "# Please download it and put it in the same directory as this .ipynb file\n",
    "\n",
    "import matplotlib.font_manager as mfm\n",
    "\n",
    "font_path = \"simhei.ttf\"\n",
    "prop = mfm.FontProperties(fname=font_path)\n",
    "\n",
    "# Add figure and axis\n",
    "fig3 = plt.figure(figsize=(12,9))\n",
    "axis31 = fig3.add_subplot(1,1,1)\n",
    "\n",
    "# Box plot\n",
    "axis31.boxplot(freq_sorted[:20])\n",
    "\n",
    "# Title, xlabel, ylabel\n",
    "axis31.set_title(\"Group of Radicals (X) - rank of frequency (Y)\", fontsize=\"large\")\n",
    "axis31.set_xlabel(\"Most frequently used 20 Radicals (count of kanjis with that radical)\", fontsize=\"large\")\n",
    "axis31.set_ylabel(\"rank of frequency used in newspaper\", fontsize=\"large\")\n",
    "\n",
    "# Xticks, labels\n",
    "xticks31 = []\n",
    "for i in np.arange(20):\n",
    "    xticks31.append(\"{} ({})\".format(rad_sorted[i], len(freq_sorted[i])))\n",
    "    \n",
    "axis31.set_xticks(np.arange(1,21))\n",
    "axis31.set_xticklabels(xticks31,rotation=45,fontdict={'fontproperties':prop, 'fontsize':14})\n",
    "\n",
    "# Flip the box y-axis\n",
    "axis31.set_ylim(axis31.get_ylim()[::-1])\n",
    "\n",
    "# avoid output from set_xticklabels\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We focused on the most frequently used 20 radicals. However, their ranges are quite uniform distributed, that we could hardly find some relationship from frequency of these kanjis with their radicals. This also tells us that even the most frequently used radicals could have some less frequently used kanjis (something difficult)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difficulty Level (grades, X) - number of Kanjis in that level (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all JLPT levels\n",
    "jlpt = ['N5', 'N4', 'N3', 'N2', 'N1']\n",
    "\n",
    "# Initialization\n",
    "container4 = []\n",
    "\n",
    "jlpt_char_count = []\n",
    "\n",
    "# For each JLPT level\n",
    "for jlptnum in np.arange(len(jlpt)):\n",
    "\n",
    "    # Locate all kanjis with that JLPT level\n",
    "    raw_in_jlptnum = raw[raw[\"jlpt\"] == jlpt[jlptnum]]\n",
    "\n",
    "    # Count the number of kanjis with that JLPT level\n",
    "    jlpt_char_count.append(raw_in_jlptnum[\"frequency\"].count())\n",
    "    \n",
    "    # Append those kanjis' frequencies to the container\n",
    "    container4.append(list(raw_in_jlptnum['frequency'].dropna()))\n",
    "    \n",
    "container4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add figure, axes\n",
    "# This part contains two axis comparisons: The Grades in part 2 ('grade 1, grade 2, etc.') and JLPT levels done above\n",
    "fig4 = plt.figure(figsize=(12,4))\n",
    "axis41 = fig4.add_subplot(1,2,1)\n",
    "# Grades: bar plot of kanjis studied in that grade\n",
    "axis41.bar(np.arange(len(grades)),grade_char_count, alpha=0.5, color=\"blue\", label='Kanjis learned in that grade')\n",
    "\n",
    "# Grades: plot of cumulative kanjis learned from grade 1\n",
    "axis41.plot(np.array(grade_char_count).cumsum(),color=\"green\",label='cumulative Kanjis learned')\n",
    "\n",
    "# Titles etc.\n",
    "axis41.set_title(\"Grades (X) - number of Kanji learned (Y)\", fontsize=\"large\")\n",
    "axis41.set_xlabel(\"Grades\", fontsize=\"large\")\n",
    "axis41.set_ylabel(\"rank of frequency used in newspaper\", fontsize=\"large\")\n",
    "\n",
    "xticks41 = []\n",
    "for i in np.arange(len(grades)):\n",
    "    xticks41.append(\"{} ({})\".format(grades[i], grade_char_count[i]))\n",
    "    \n",
    "axis41.set_xticks(np.arange(len(grades)))\n",
    "axis41.set_xticklabels(xticks41,rotation=30)\n",
    "axis41.legend(loc='best')\n",
    "\n",
    "# Axis 2: kanjis and JLPT levels\n",
    "axis42 = fig4.add_subplot(1,2,2)\n",
    "\n",
    "# JLPT: bar plot of kanjis studied in that level\n",
    "axis42.bar(np.arange(len(jlpt)),jlpt_char_count, alpha=0.5, color=\"blue\", label='Kanjis learned in that level')\n",
    "\n",
    "# JLPT: plot of kanjis studied till that level\n",
    "axis42.plot(np.array(jlpt_char_count).cumsum(),color=\"green\",label='cumulative Kanjis learned')\n",
    "\n",
    "# Titles etc.\n",
    "axis42.set_title(\"JLPT Level (X) - number of Kanji learned (Y)\", fontsize=\"large\")\n",
    "axis42.set_xlabel(\"JLPT Level\", fontsize=\"large\")\n",
    "axis42.set_ylabel(\"rank of frequency used in newspaper\", fontsize=\"large\")\n",
    "\n",
    "xticks42 = []\n",
    "for i in np.arange(len(jlpt)):\n",
    "    xticks42.append(\"{} ({})\".format(jlpt[i], jlpt_char_count[i]))\n",
    "    \n",
    "axis42.set_xticks(np.arange(len(jlpt)))\n",
    "axis42.set_xticklabels(xticks42,rotation=30)\n",
    "axis42.legend(loc='best')\n",
    "\n",
    "# Aviod legend output\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This is a 2-axis figure, comparing two different grade criterias: Japan's elementary/secondary school grades and JLPT level (Japanese Language Proficiency Test - for Foreigners). In the grade axis, the number of kanjis student study in elementary schools are quite uniform, but there's a significant increase in junior high school. This is reasonable: Junior high contains 3 years. Unfortunately we cannot find some more specific data, but the mean of kanjis in junir high, 1031/3 = 344, tells us that junior high students are learning more kanjis than elementary students per year.\n",
    "\n",
    "JLPT levels are facing the foreigners. Similar to the grades axis, there is a significant increase in N1 level, which is the most difficult level. Kanjis taught by JLPT level is not perfectly distributed, and the upgrade from N2 to N1 is the most difficult. \n",
    "\n",
    "When comparing the cumulative curve, we found that the two curves are quite the same shape: that means an N2 learner might have the same level as Japan's elementary school graduate, while N1 learner would have the same level as a junior high graduate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
